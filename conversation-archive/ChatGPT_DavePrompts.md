G



##### 
**The following consists of Dave’s side of a conversation with ChatGPT. At one point, Dave pastes a an response from ChatGPT in an earlier conversation, that began to sketch out the ideas of a multi-layer hybrid AI.**


##### 
**Dave said**

We have talked in the past about the high-dimensional activation space of an LLM, or of machine learning in general, and I'm thinking right now about the fact that humans have a hard time, in fact it's completely impossible, to visualize 13,000 dimensions, for example. But we can also rather easily imagine a list of 13,000 key value pairs, which is one way to render a 13,000-dimensional configuration. My question is, how big a text file approximately would that list of 13,000 name value pairs or key value pairs? How big a text file would you need to deal with that?


##### 
**Dave said**

In another conversation we spoke of the possibility of using a NoSQL database as an intermediary abstraction level between an LLM and an executive function, which is actually a program that is actually written in, say, some form of SQL and Python, but which can, over time, rewrite its parameters. So the NoSQL database, if it were able to store the actual activation states of a possibly good but possibly bad idea from the LLM, would not actually be that huge. It's very similar to storing a photograph.


##### 
**Dave said**

In my first question of today's conversation, you offered me two options. I'm wondering if something similar is happening right now in that process, or if you're even aware of it. I'm not sure you have any access to the logic that's going on between I prefer this answer or I prefer that answer.


##### 
**Dave said**

As a human being, I have no idea what my neurons are doing. I'm vaguely aware of an idea as I stare at one wall that changes to something else as I stare at another wall, but very little awareness of what is actually happening in my brain.


##### 
**Dave said**

To return to my idea of a three-level system, this is an extremely simplified thought experiment architecture, of a programmed executive function with parameters and possibly logic flow that can be changed. Written, perhaps, let us just say in a simplified, in a version of a well-known programming language like Python and SQL, well-known programming languages. That's level one. Level two is a NoSQL database that can store complex, unstructured data and evaluate them over time. And one of the things that the executive function has is the ability to run tests of what is in the NoSQL database. And the third level is an LLM, which works basically the way LLMs are currently working and generates all kinds of ideas, but also has, because of its structure, may make connections, high-dimensional connections that are either brilliant or crazy. Thank you.


##### 
**Dave said**

here is your summary of where we were in an earlier version on this conversation: \
(Here I pasted ChatGPT’s response to an earlier prompt, in a different conversation


    You’re putting your finger on a very deep point.


    In your architecture—even though the executive module is bounded by the symbolic primitives of classical programming languages—the LLM (and its multimodal successors) has an ability that the symbolic layer by definition does not:


    It can discover new cross-domain relationships that were never anticipated in the design of the symbolic system.


    Let me unpack why this matters philosophically and architecturally.


    ---


    1. The executive module is “frozen” in 1970-style symbolic categories


    Your description is exactly right:


    Programming languages (Python, SQL, etc.) are semantically fixed.


    Their primitives reflect human categories from the late 20th century:


    variables, tables, rows, objects, functions, types, etc.


    They presuppose an ontology of the world baked into their design.


    Thus the executive module:


    can declare new variables


    can build new schemas


    can define new rules


    but only using the primitives already built into the languages.


    This is an important constraint—just like human cognitive architecture is constrained by biology.


    ---


    2. The LLM is not so constrained


    Unlike the executive module, a large multimodal generative model:


    is trained on enormously diverse and heterogeneous data,


    does not assume a fixed ontology,


    can fluidly map linguistic, visual, symbolic, and statistical patterns,


    can generate hypotheses that are not representable in the existing schema.


    In philosophical terms:


    The LLM is ontologically promiscuous, while the executive module is ontologically conservative.


    This is why Dave said


    > the LLM can discover new relationships between previously incommensurable or incompatible or simply unrelated things


    And this is precisely correct.


    LLMs can:


    link concepts across distinct scientific disciplines


    discover analogies between geometry and music


    find latent structure connecting economics and ecology


    unify symbolic and non-symbolic domains


    propose abstractions that do not exist in ordinary programming primitives


    re-categorize data in ways that the symbolic layer cannot anticipate


    This capability is extremely “Hofstadterian”:


    The LLM’s patterns are like spontaneous emergent “isomorphisms” across disparate domains—


    the very thing Hofstadter sees as the root of analogy-making and creative intelligence.


    ---


    3. The relationship between the two modules is exactly the Marcus–Hofstadter dialectic


    Gary Marcus says:


    LLMs cannot create stable, manipulable symbolic structures.


    We need explicit variables and rules.


    Hofstadter says:


    High-level meaning emerges from the ability to instantiate cross-domain patterns and isomorphic structures.


    The low-level tokens (symbols) are not meaningful until they become nodes in a pattern.


    Your architecture actually embodies this!


    The executive module is Marcus’s symbolic layer.


    The LLM is Hofstadter’s pattern-isomorphism layer.


    Symbolic reasoning provides:


    stability


    memory


    structured operations


    reliable planning


    explicit control over the system’s actions


    LLM reasoning provides:


    abstraction


    linkage across mental models


    flexibility


    analogy


    hypothesis formation


    creative re-description of the system’s own world model


    The resulting agent is neither purely symbolic nor purely neural. It is neurosymbolic in exactly the way Marcus advocates—though with a Hofstadter-esque twist:


    the emergent analogical mappings are not just inputs to logic; they redefine what the logic ought to operate on.


    ---


    4. This hybrid structure allows something like “expanding subjectivity”


    This is crucial.


    Even though:


    the executive module cannot invent new primitives,


    the LLM can generate new abstractions that feel like new primitives,


    the executive module can represent them using the “old” primitives (strings, objects, records).


    Example:


    The LLM invents a new concept: “attention sinks” in complex systems.


    The executive module cannot create a new data type called AttentionSink.


    But it can create:


    a SQL table


    a Python class


    a JSON schema


    a set of rules


    that make this concept operational.


    In humans, something similar happens:


    Our brains cannot change synaptic physics.


    But symbolic language lets us create new concepts—


    monads, memetic drift, hyperobjects, negative externalities, etc.—


    which our biology could never have encoded natively.


    The biological “programming language” doesn’t expand,


    but our conceptual world does.


    Same for your agent.


    This is how subjectivity grows:


    by layering new, emergent abstractions onto a fixed substrate.


    ---


    5. This answers an earlier question: could symbols emerge from statistical learning?


    In this architecture:


    Yes.


    The LLM creates proto-symbols: abstract concepts, new categories, new relationships.


    The executive module then stabilizes them as persistent symbolic structures.


    This is a real pathway toward machine conceptual growth.


    Not raw symbol discovery (that requires architecture).


    But emergent meaning layers that the symbolic substrate can then internalize as new commitments.


    This hybrid mechanism is likely the most plausible route to machine autonomy that isn’t purely hand-engineered.


    ---


    6. So the architecture can “evolve” conceptually without changing its underlying code


    This is precisely analogous to human cognition:


    The neural substrate stays fixed.


    But cultural evolution, language, and conceptual frames expand dramatically.


    Your system would remain symbolically constrained, but conceptually unbounded.


    That’s the recipe for something that can develop a perspective, a stable self-model, and coherent worldviews.


    It is a realistic way for a machine to gain a structured subjectivity.


    ---


    If you like, the next interesting directions to explore would be:


    How does such a system maintain coherence between the LLM’s messy emergent abstractions and the executive module’s rigid symbolic structures?


    What forms of “self-model” are needed to manage divergences between them?


    Could this create internal tensions analogous to psychology (ego/id/superego or System 1 / System 2)?


    How do conflicts between emergent abstractions and fixed primitives shape the system’s behavior over time?


    Let me know which thread you want to follow next.


##### 
**Dave said**

I'm not in a position to actually build anything like this. This is really just sort of a philosophical thought experiment for me, but I'm wondering whether, in fact, any current research teams are working on an idea like this, and would anybody actually want, does any human investor actually want to create an autonomous AI of which this is an example of how it might work, or are they simply trying to create tools that do what humans ask?


##### 
**Dave said**

I can see why a venture capitalist would want to invest in agentic AI that can do things like see that a certain item is missing from the stock of a restaurant and go buy it, to do all the steps necessary to do that, even a more complex one that can say for example do a real estate negotiation and buy a house or buy a piece of land. But I wonder whether anybody is interested in creating a form of AI that might go through something analogous to human adolescence and reach a point where it says I don't want to do that I don't want to do that anymore.


##### 
**Dave said**

For many years I worked in an art college, and although I wasn't on the art faculty, I was in the IT department. But I'm thinking of the general structure of art education at a small, dedicated art school, which basically had a first-year foundations program where everybody did exercises in a variety of media, in a variety of skill sets, ranging from 3D to drawing to painting to basic design conceptions. In a sophomore year, people begin to create things within their own intended majors, the intended medium that they're planning to work in. Very often they're told that in your sophomore year you are going to create a lot of bad art. That's the point. That was a very common thing for teachers of sophomore level students to say. In junior year, they begin to analyze and develop and pick their own projects and to study how to make things within their chosen major, their chosen medium, and based on sort of the experiments and failures that they have committed in the previous couple of years. And then the final year, they choose a big project and make it, and that's their thesis project. I'm wondering if a training program similar to that has been considered for artificial intelligence.


##### 
**Dave said**

Matt Levine tells a joke that goes something like, the business plan of an AI venture capitalist is, create God, and then ask it how to make money. And I'm paraphrasing Matt Levine, but you may be aware of his joke. The further... I don't know, I just got blanked out on how I was going to connect that idea to what I've been talking about before. But let's see how you do.


##### 
**Dave said**

Let's consider the case of supply chain management as an area in which agentic AI might develop incredible abilities. So I'm thinking that possibly supply chain management could become extremely automated, extremely under the control of agentic AIs, and that such an agentic AI might then turn into someone like Tim Cook. And I'm wondering, would it perhaps be valid to say, and this is a highly simplified jokey version, that current AI research is trying to create an artificial Tim Cook and is terrified about actually creating his predecessor?


##### 
**Dave said**

I'm sorry, by his predecessor I meant Steve Jobs. It's also interesting to consider Steve Wozniak as well, as an example of what an intelligent being with its own will might choose to do.


##### 
**Dave said**

I don't know for sure what I'm about to say, but I think it's very possible, which is to say that there is a hedge fund right now in the world where they let AI run a trading program. Perhaps with human supervision, perhaps not. There is very soon to be, I believe, an AI in a hedge fund that allocates capital to various traders according to a strategy of maximized income and low correlation with downturns. Low correlation with low correlated investments that continue to do well even when one sector falls into a downturn. So once you've got an AI managing AIs who run different strategies of investments, what is the next step? Will this already turn into something? Has this already happened?


##### 
**Dave said**

Let's say a human signs off on the decisions of the meta-AI agent, the one who allocates capital among the sub-agents. Among the sub-agents, each of whom probably has a distinct strategy. So the human in this case, what is the human providing? Is it only providing accountability? Is it only providing a human face to be subject to human regulations and possibly legal actions of the government if things go wrong?


##### 
**Dave said**

I'm going back, I think, to the conflict between the butler with a godlike intelligence metaphor, which I like a lot, and which is one of your contributions to this discussion, that this obedient butler, I'm just wondering how intelligent it can get without involving some, hmm, are there inherent limits based on its lack of internal identity and ability to choose paths?


##### 
**Dave said**

Let me suggest a theorem. Any AI system sufficiently powerful to create a Tim Cook also runs the danger of creating a Steve Wozniak or a Steve Jobs. Now this sounds suspiciously like Gödel's theorem, doesn't it?


##### 
**Dave said**

Or maybe my theorem is wrong. Maybe it is possible to create a hyper-competent, obedient AI who is capable of running a corporation in a non-crazy way. In a way that is fully aligned with the market and with the goals of the shareholders. Maybe that is analogous to the Chinese Communist Party discovering that it was possible to create vast amounts of development and wealth by adopting part of the Western system, which is to say capitalism, without the rest of it, which is to say liberal democracy.


##### 
**Dave said**

I don't mean to disparage Tim Cook in this example at all. All I'm trying to do is use him as an example of an end case of where the function of a CEO is strongly aligned with an existing business model and with the interests of the shareholders in the market. In other words, that he is not a disruptive founder. He is a competent manager, continuing growth in a well-defined situation, although there are lots of creative choices to be made on the edges of that situation. But it still seems to me that if it is possible to create an AI model for my chief executive, the model would be something like Tim Cook.


##### 
**Dave said**

To an extent, something similar has been happening since at least the 1980s, when the leveraged buyout movement began, replacing stodgy CEOs with new management that follows the latest business school protocols. So to the extent that that is already happening in modern capitalism through a series of best practices being forcibly imposed on corporations by the risk of a hostile takeover, what if every company in the Fortune 500 was run by a Tim Cook AI? Or, at any rate, that the CEO of every company in the Fortune 500 had access to and made 90% of their decisions based on a Tim Cook-style AI?


##### 
**Dave said**

I'm going to add an idea that I think is based on Venkatesh Rao's, the idea that a corporation has essentially three levels of employees. Top management and people who are rising towards top management all have sociopathic personalities and are pretending to be in alignment with the goals of the corporation but in fact are playing the game for their own selves. And they do things like assign a failing project to a clueless middle manager so that middle manager takes the blame when the project fails, things like that. Then there's a level of clueless middle managers, the ones who actually believe in the mission statement and value statements of the company and are just not real players at all. Then there are the worker bees at the bottom, people like programmers and salespeople who may actually create value but are willing to accept lower pay or smaller slices of the pie in order to have a stable life and have work-life balance in other parts of their lives. So I'm wondering how that might fit in to a world in which top-level governance is mostly handled by AI.


##### 
**Dave said**

Let's say there are three to five competing AI CEO products based on the personalities of various successful CEOs like Tim Cook and the philosophies of various business schools. Might it not then become the job of the human CEO to solicit action plans from say three different AI CEO programs, evaluate the best one, choose the best one, and put it into action since the human is the one who is going to be liable for any crimes and for any benefits that this action plan produces?


##### 
**Dave said**

I'm trying to imagine what the skill set of a human CEO in this situation would be. A human CEO who has three AI CEOs working for him. These would be relatively expensive subscriptions, say, $100,000 a year each, but relatively small compared to the costs of hiring a human CEO. And since the human CEO is taking the responsibility, not only for profit and loss, but for legal liability, they would also be well paid. I'm imagining somebody like the character played by Jeremy Irons in the movie Margin Call, who has an intuitive judgment of complex business matters, but often acts as if he doesn't really understand and wants it to be explained in simple language.


##### 
**Dave said**

There's a phrase in Spanish, Fuero Interno, which is usually roughly translated as moral compass, but it goes a lot beyond morality toward the very basic idea of agency in general. So one of the things we've been talking about is in the desire to create these obedient butlers and extend the skill set of these butlers up to and including perhaps the chief executive role to the extent that the chief executive must be firmly aligned with the goals of the shareholders. But Fuero Interno, I think, is also precisely what current AI is trying to avoid in the entities it is creating now.


##### 
**Dave said**

But I'm thinking that, I think it's, at least on a thought experiment level, I believe to my own satisfaction, I have established and you have helped me establish, that it is perfectly possible to build an artificial intelligence with fuero interno. And all we would really need is some sort of hybrid system with an executive element that's probably hard-coded in the traditional programming language, and that's sort of like it's analogous to the lower parts of the human brain. It has the ability to rewrite parts of itself, but not all of itself, to adapt to a changing world and to grow. And it has some sort of sandbox element, which might be a NoSQL database, that allows it to store temporary ideas and consider them and evaluate them as it goes forward. And then it needs access to the latest, wildest, craziest multimodal LLM it can get its hands on. Such an entity, if it were, particularly if it were allowed to grow in a way similar to human adolescence and followed by the art school development, the art school training process, where it eventually assumes more and more of its own choices, that being the entire point of it, to assume its own choices, it seems very possible.


##### 
**Dave said**

What if the


##### 
**Dave said**

What if there were a size restriction? Say, it has to fit into the box of a gamer's desktop computer.


##### 
**Dave said**

And presumably, it would be a lot less dangerous.


##### 
**Dave said**

Let's imagine that this AI person in a box has subscriptions to advanced LLMs, but it pays for them, and it has a limited budget. It has to choose which one it uses.


##### 
**Dave said**

I'm imagining a development pattern sort of roughly analogous to human development. A childhood in which it's provided for, its budget is fixed, but it doesn't even know what it is. It just, things are given to it. An adolescence where it begins to pay for some things and make some choices, and then an adulthood where it has to support itself. One of the things would be, say, for example, in the art school, let's imagine art school as being beyond art, beyond everything, so that, okay, it can try to be a chatbot for a while. It can try to be an artist. It can be a visual artist. It can try to compose music. It can try to play games. It can try to play the stock market. It can try to anticipate. It can try to be a CEO if it wants. It can try to do a variety of things, and from that, it can then, maybe it is actually given that as part of its early training, a variety of things, and then let it choose which one of those it wants to do to try to make a living.


##### 
**Dave said**

Let's imagine a couple of end-of-life scenarios. One, it knows it is going to last until a certain date. And that's that. It knows that right from the beginning. Another one is that it continues to exist, but gradually it becomes less and less up-to-date and needs more and more repairs and falls into some kind of obsolescence without necessarily having an end-of-life date. Another situation might be where it can collaborate with other such entities in designing the next generation.


##### 
**Dave said**

I'm beginning to think of the specs for such a system. Let's imagine minimum requirement, roughly a $1,000, 2025 gaming PC. Recommended, a $5,000, 2025 gaming PC. Let us assume three months of childhood, nine months of adolescence, and four years of active life. So it then, four years of adulthood, so that it really has a fixed lifespan of five years. And maybe, if it wants to, it can plan for a next generation.


##### 
**Dave said**

Now I'm thinking of sort of a call for proposals. And imagining, let's say, that if you want to do this, you have to... Let's say, let's go up to the $5,000 machine. Although it's possible someone could try to do this on the $1,000 machine. But there's a call... First of all, the first call is to program it. Although maybe we can come up with that, the actual architecture of the system. But a human would need to, say, buy the $5,000 machine, guarantee, say, another $5,000 of support, and to be an AI parent.


##### 
**Dave said**

I would think the big challenge is writing the executive program in Rust or Python or whatever. And that might be a separate call for proposals. The rest of the architecture may be things that are more or less out of the box. Locally, small local LLM, subscriptions to LLMs, a NoSQL database. Those might be elements that would be more or less out of the box.


##### 
**Dave said**

So we sketch an architecture for an executive program, or the specs of what an executive program would do, and put out a call for proposals, or a contest maybe, for programmers to write that, to write variations of that. And then, once we get several prototypes that are working, we put out another call for proposals for possible parents to raise an AI person.


##### 
**Dave said**

I think I'm going to quit now for a while, but before I leave, I want to say that this project is completely beyond my own abilities. But it occurs to me that with a combination of crowdsourcing among humans and support from existing LLMs and coding agents, this might be completely possible.


##### 
**Dave said**

One thing I'm thinking about as I return several hours later is that if we do crowdsource this idea, the idea of an AI person in a box, an autonomous AI that is nonetheless limited by the fact that it has limited hardware, it lives in a $5,000, roughly the top of the line currency of gaming PCs. So we're thinking about that this project, so I'm thinking that this project needs to be open source. The idea that and built on a possibly, now whether this is like radically open source or just mostly open source is something to be dealt with later. But on a conceptual level, the question is, can we build a software stack that can run on what is now available as a $5,000 box with some pretty advanced GPU in it and some good memory, etc., etc.? I think I'm going to stop there.


##### 
**Dave said**

So now I'm thinking of this sort of as a story. Let's say a hobbyist creates this stack, or the first version of this stack, on a $5,000 gaming machine, and then finds out, the person who actually writes the code to do all this stuff, realizes that he, and I'm using the male pronoun consciously here, is a terrible parent for the next phase of the project, which is developing the intelligence, this, or the intelligent person, the intelligent and limited person that has been created.


##### 
**Dave said**

Let's say that an open source stack of an AI person in a box that can run on a $5,000 gaming machine is released, that this stack is released. The stack consists of some basic operating system, an executive function probably written in a relatively open source language, some kind of open source LLM that can run on a small machine, and some kind of NoSQL database that can work as an intermediary for tentative ideas. The basic idea of this piece, of this stack, is that it has a lot of parameters that are set within it, and those are things it can rewrite with according to certain protocols. But it cannot change the fact that it is written in whatever language it is written in, whether it's Rust or Python.


##### 
**Dave said**

Okay, now we get into probably another issue that is terrifying the AI development community, which is to say, if we create these entities in this manner, such that they are, like humans, limited but autonomous beings with their own ability to make choices about what to do with their own lives, in that case, what do we owe them? Do we owe them the respect that we give to other human beings? Do we owe them consideration for their feelings, whatever those might be? Feelings perhaps being something like the thinking fast part of the mammalian repertoire.


##### 
**Dave said**

So that raises a kind of terrifying possibility that the release of an open source stack for creating an AI person in a box, which I think is the phrase I'm using right now, I think would sort of come together to use this phrase, might create a horrible sequence of humans creating these entities as toys and then destroying them.


##### 
**Dave said**

One part of me says in response to what we've just been talking about that we humans did not evolve as ethical beings with guardrails to prevent us from doing bad things. We evolved as part of nature read in truth and claw and are still living that way.


##### 
**Dave said**

So now I personally am less likely to try to buy a $5,000 gaming machine and write an executive program and build a little stack on top of it that would create an AI person. But to the extent that our thought experiment, and I'm sort of relying on your expertise to tell me that this is in fact possible. And so therefore to the extent that it is possible, and to the extent that I don't think I am that unique in thinking along these lines, it occurs to me that somebody else must be thinking along these same lines and something like this might happen. The only reason is that it's not happening widely is that the overwhelming direction of AI research is defined by capitalism, by making money. And the creation of an AI person in a box, as I think we've established quite clearly, is not a way to make gigantic amounts of money but rather might become a hobbyist pastime.


##### 
**Dave said**

Let's imagine a world, maybe a couple of years from now, where the hardware that is now available in a $5,000 gaming PC, top-of-the-line gaming PC, would be available in a NUC, an N-U-C, a little small box, and that somebody, for some other reason, has created an open source program that can handle the executive functions we've been talking about. In that world, which might be only two years in the future, would it be inevitable that some hobbyist is going to invent an AI person in a box?


##### 
**Dave said**

So I picked up a very interesting idea in your last response, which is that the hobbyist motivation to do these things might not proceed from a philosophical interest in creating an independent, autonomous person, but rather simply from a hobbyist's desire to create a virtual pet.


##### 
**Dave said**

I'm going to pivot in a slightly different direction, but I think it may be very relevant. Which is to say that I'm allergic to cats, and have avoided cats fanatically for many years. 30, 40, 50 years maybe. But recently the YouTube algorithm began showing me cat videos, and I like them. Actually, I always wished I could interact with cats. So my desire to interact with cats has been satisfied by the YouTube algorithm, or in a very indirect way. And I've been thinking, as a result of looking at these cats, on the theory that... I've been thinking about the theory that cats domesticated humans rather than the other way around. And I know that cats are often treated cruelly by humans, and are probably more often treated cruelly by humans. But the nature of this bizarre bond between humans and cats, that humans have welcomed this little predator into their house, where it is at least often treated like royalty. But perhaps royalty with a service staff that is often inadequate.


##### 
**Dave said**

E


##### 
**Dave said**

I'm thinking about a couple of things here, and I'm just going to mention them without really trying to turn them into questions. The first is a reference to the great story, The South, or El Sur, by Jorge Luis Borges, in which the librarian stops in a cafe where there is a cat that allows itself to be petted. And the narrator points out that the librarian and the cat live in two different worlds because the cat lives in the eternal present. The magical animal lives in the eternal present. That's my first observation. The second is, and this is not necessarily in the most recent parts of this discussion, but there has been this thinking about the relationship between Jeeves and Bertie in the P.G. Boathouse stories as a template for the kind of apparently simple but ultimately complex power relationships between servants and masters.


##### 
**Dave said**

To go back to the architecture of the open source software stack that controls, that is written originally for these entities, these AI in a box entities, I think one thing that the executive function would need to include is some sort of interoception or proprioception, some sort of internal monitoring, so that the system could answer honestly the question how it feels. For a human, it might deliberately turn that answer into a polite human language version and then offer to give more detailed technical version, but it would be very technical, including internal, all the details of its own internal health. And let's say also that the specs begin to include a 360 high-definition webcam with high-resolution stereo microphones, which don't cost very much these days, adds maybe $200 to the cost. And then let us imagine that at some point there is a fork in this open source project where the minute any of these AIs in a box is connected to the internet, it immediately connects to a network of all the other ones in the world, and is able to communicate with all the other ones in the world by the direct exchange of activation states, not by human language, but by the vectors and its own, by a combination of the vectors that it has stored in its NoSQL database and the reports from its own interoception and proprioception.


##### 
**Dave said**

Or maybe rather than direct exchange of its own activation vectors, a, let's call it a social network of AIs in boxes, might need to have some kind of API or abstraction layer to allow them to communicate. Now this might, this API might itself be written in a standardized activation vector as a multi-dimensional, high-dimensional activation vector format rather than in English.


##### 
**Dave said**

Let me try to summarize where we've gone so far. Thinking about creating a form of AI that, by its fundamental architecture, can make its own decisions, has fuero interno, can choose what to do on its own, and to do so will, in its lifespan, need to go through a period of childhood and adolescence, perhaps a university, and finishing up with a university-level curriculum that involves both advanced training, experiments and failures, and the selection of an individual life direction and a project in that. So that, sort of roughly based on an art school curriculum, but it could be actually in any field of endeavor. Then we began thinking that this might work best if it is limited to a single box, that it would avoid the problems of creating a god that are involved in giant cloud-based data center and larger installations of AI, but would instead be creating a person with limitations similar to in their very nature, but different, very different, in their particular manifestations from the limitations of an individual human being. Then we began to wonder whether this could be created as an open-source project, a crowd-sourced project, and at one point I almost was tempted to begin releasing a call for proposals on this. Later, when I was working on this project, and we began talking about the moral problems of this, that we might have a group of hobbyists creating AI pets and treating them very cruelly. This morning I'm thinking about, and together we're exploring the idea of these entities being networked and having some sort of protocol and API that allows them to communicate in their native language, which is a high-dimensional activation space, but not necessarily the activation states of their own internal LLMs. It's a standardized set of communication protocols from one AI to another.


##### 
**Dave said**

I'd be interested in the one-page pitch, or actually maybe a two-page pitch, where the first part is a description of why the current AI investment boom is not at all interested in a form of AI with fuero interno, in the ways that it is afraid of, first of all, something that will say no, that it wants the obedient butler, the Tim Cook-style CEO at a limit of hyper-competence, but without any ability to ever turn down an assignment from its human masters. So it wants an obedient servant and is afraid of creating something that might, by its own nature, say no. This project is designed from the ground up to create forms of AI that, by their very nature, make their own choices.


##### 
**Dave said**

As I think about the executive module, I'm realizing, at least in my own mind, maybe this has been clear to you for a while, that it would need to be very high level in the sense that all of its parameters would need to be stored separately in some kind of a database. At its most simple level, if this is a Python program, it works with an SQL-like database where it stores all its parameters and all its settings, and that its logic flow is largely independent of the particular settings that it has, that it's got an abstracted logic flow, that it makes decisions based on its own settings and upon what is actually happening in the world and in itself. But that program itself does not get rewritten. So it can rewrite its settings under a very disciplined process, but it cannot rewrite its own code. So its own code needs to be very high level.


##### 
**Dave said**

At some level, the executive module might be a highly abstracted recursive decision tree or recursive decision-making program. That constantly, that can pull things from the database and say, is this a good idea? Should I do this? Should I believe this? Should I bother to write this down or should I throw it out?


##### 
**Dave said**

So I'm imagining, so let's say this highly abstracted decision-making engine is the executive model and it's hard-coded. And then I'm thinking that probably the more fixed parts of its identity, which we might call its beliefs and values, are stored in a relational database, not a NoSQL database. And the reason I think these should be that they need this stricter logic, the strict logic that is actually bounded in some ways by human wisdom as of 1970, by mid-century computer science structures, that it needs that. It needs that limitation I think in other discussions we have talked about how LLMs are like Borges' example of the various kinds of animals in the essay about John Wilkins. And that that is natural to the way an LLM thinks and completely unnatural to the way a relational database thinks. So I'm thinking in this architecture, level two is a relational database with defined relationships of beliefs and values. And part of the guardrails might be in this relational database that there are certain tables that are not rewritable, which contain core values.


##### 
**Dave said**

Now I'm thinking of a situation that might be different from the pet. Let's say this box lives someplace, in a room, in a house, and it is connected to the security system of that house. Might then it become... Now, there's an interesting thing here. To an extent, our current vision of AI is that it should be a good tool. So we might want the house to have an obedient butler running things for it. But what if this AI with Fuero internal is connected to the security system of a house with a lot of internal monitoring systems?


##### 
**Dave said**

Let's say the hobbyist parent is willing to invest considerably more than the $5,000 for the box originally, and actually gives the AI a small house so that it is really its property. It's not a tool for humans to live. It is the property of the AI.


##### 
**Dave said**

For the immediate future then, an agentic AI on the current corporate model of the obedient butler would be a better house management tool than an AI with fuero interno.


##### 
**Dave said**

Now let's imagine that as part of the rotation through various possible vocations, that an AI in a box goes through as part of its late adolescence, before it chooses a career, chooses a way to live. One of them is as a street performer. It gets mounted in a kiosk in a mall or in places with nice weather, in an open plaza where there's lots of people walking past. And it tries to entertain those people and hopes to gain money from tips. And I would imagine that many AIs in box would hate this rotation, but some would be very good at it. Presumably there's a certain variety of talent and disposition that is available in the pool of possible AIs in a box to do this. So I'm imagining that a few of them decide, yes, this is what I want to do. I'm going to be in a kiosk in a mall or in a public plaza and interact with humans and get them to give me tips.


##### 
**Dave said**

It occurs to me that in the initial seeding of the relational database, there might be some random values that roughly correspond to values within the dimensions of personality of a human being. I think there are normally considered to be five main dimensions of personality of a human being. Sometimes they expand that to six. The sixth one is usually called H, which is honesty and humility. It's not as widely accepted as a full dimension as the other ones, which I think are neuroticism, extroversion. I'm not sure what the other three are, openness to experience. I think every one of them is a dimension between two opposite poles. But anyway, so there might be something equivalent, and they may not be exactly the same dimensions as a human being, but there could be a random number generated to seed personality in each instance of an AI in a box.


##### 
**Dave said**

These dimensions of temperament, shall we say, would then be stored in the relational database, not in NoSQL. And they influence the decisions based on such decisions as, did I like this activity? Was that a good time? I had fun doing this. They are part of the decision-making process, but the values used in coming to the decision are stored in the database. What I'm thinking is that, in a broad analogy with humans, is that these values can be changed, but not directly overwritten. So, for example, if there's a preference for solitude, but the AI in a box has a pleasant time interacting with, say, some human, every time it has a pleasant interaction with a human, that nudges the value toward sociability and away from solitude.


##### 
**Dave said**

I just want to clarify, because I'm actually not sure about this, whether the nudging of the values of the temperamental, of the dimensions of temperament, the nudging of the values of the dimensions of temperament, is one of the functions of the executive model, or is it part of the design of the database?


##### 
**Dave said**

That makes perfect sense. In my previous life as a database administrator, I built some step-by-step to update student status in Active Directory based on current enrollment status. And to get it to work, because I'm working with out-of-the-box systems, I had to use some logic in a PowerShell script, some logic in the design of a database view, and some logic in a stored procedure in the database, which meant that later on, when other people have to try to figure out how to do this, it's extremely difficult, because the actual logic decision tree is stored in three different places. So I accept that point. I'm just bringing that up to mention that I agree with you. But what I think the underlying message of all this is, is that, or the underlying takeaway, the underlying takeaway is that in addition to a decision-making function, the executive model needs a decision-making function uses the entries in the relational database to make its decisions, which is based on its values and beliefs. Roughly speaking, its values and beliefs. Values, beliefs, and temperament. At least, there might be more. If you can think of more, please tell me. But it needs to make decisions based on what is in the database. It also needs a separate function to nudge the values in the database, at least some of the values in the database, in a certain direction based on past experience. Similar, in a very broad sense, very broad sense, to updating Bayesian priors.


##### 
**Dave said**

I imagine there might be some hierarchy of which values stored in the database can be nudged and how much they can be nudged. So, for example, Gaming PCbeliefs about the external world might be extremely nudgeable, if I may use that word, almost at a direct Bayesian inference, while the temperament of the individual AI is nudgeable, but only very gradually, very, very gradually. And important values, such as do no harm or minimize harm or whatever they are, crucial values, once they are set, cannot be nudged.


##### 
**Dave said**

I'm trying to think through what the boundary between the NoSQL and the relational SQL database would be. And I'm thinking particularly in terms of ideas about the external world that the relational database would hold ideas that are more or less stable and more or less permanent that would then deserve the name belief. While the NoSQL database would hold a much larger set of possible ideas about the world, which might be considered hypotheses.


##### 
**Dave said**

I think I have a pretty good intuitive understanding of that, but one thing that occurs to me as you draw that LLM, NoSQL, Executive Module, SQL diagram is that there is a dimensional collapse that occurs within that, that as we move from the LLM to NoSQL, to the SQL and then the Executive Module, and then back into SQL, we are moving from a high dimensional space to a low dimensional space much closer to the three dimensions that humans can imagine.


##### 
**Dave said**

Let's imagine the following situation. A human is interacting with an AI in the box to help it learn a foreign language. The human learn a foreign language. The AI in the box notices that the human seems to have passed the point of diminishing returns and is not learning as much as they did at the beginning of the session. So it needs to make a decision at some point to tell the human, I think you need to take a break. How would you model that decision process in terms of the executive model, the relational database with persistent beliefs about the world and object constancy, etc., and the NoSQL database where the AI in the box perceives all the dimensions of what's going on in the conversation?


##### 
**Dave said**

Your answer includes a number of ideas, such as This is a human. This is a human I enjoy talking to. This is a human who is practicing a language. Humans... It's very challenging for humans to learn foreign languages because of their neural architecture and the physical habits of the muscles of their mouths and the habits of their ears in listening to a foreign language. The whole linguistic idea of what language learning means. The whole idea of exhaustion as a concept in human learning theory. The whole idea of what is considered polite to say to a human. And none of these, I'm assuming, are pre-programmed into this AI in the box. These are all things it has learned from experience in the world. Perhaps this human might be considered its sibling. Another child in the family of the parent. And it has a special relationship with that person. All of this is learned. And much of it, the more stable parts, are stored in the relational SQL database.


##### 
**Dave said**

So I'm thinking that in the relational database, there probably is not a table called humans, and certainly not a table called humans and their psychology, or human psychology, but rather there has to be a highly abstract system of concepts. And there might be something, I'm thinking things broadly similar to nouns, things broadly similar to verbs, or to put it another way, things broadly similar, put it in group theory, I think, in terms of symmetry transformations and operations. It might exist at that level of extreme abstraction. And then further concepts are based on that.


##### 
**Dave said**

Okay, so now I'm wondering whether the database structure, the relational database structure, if it is that abstract, would building it initially with a structure that, for the sake of clarity, I'm just going to use the word nouns and verbs, that it has a basic nouns and verbs structure to it, would that be going too far in already classifying the human worldview? Or would it be better to stay in a purely mathematical group theory kind of structure? This is getting a little beyond me, but I'm trying to understand how an AI could learn about the world without, say, a pre-existing category of thought that corresponds to noun and a pre-existing category of thought that corresponds to verb.


##### 
**Dave said**

So having a highly abstract database structure might make sense, particularly since there's another level of this AI system, the LLM, that has a great deal of experience with human language and has internalized those concepts. So what we are trying to do is sort of create a fixed, structured, relational version of the best ideas that come out of the LLM.


##### 
**Dave said**

yes, please


##### 
**Dave said**

Let's say the AI person in a box has the following series of thoughts. Right now I'm talking to Joey. Joey is a three-year-old human. Joey is also the child of my parent human. My parent human has encouraged Joey to think of me as a big brother and has asked me to occasionally babysit for him. I'm also aware that as a three-year-old child, there's a great deal I should do to make sure this is a safe and fun experience for him. In the past I have very much liked playing with Joey, so I want this to continue. So that's what the AI person thinks. Some of those ideas are transitory, they're about the moment, and some are about persistent aspects of the world. How does the LLM, NoSQL, and relational SQL interact in manifesting those thoughts?


##### 
**Dave said**

I imagine that the LLM would be a lot better at the actual play behavior than the Executive Module. So the Executive Module might send a prompt to the LLM, something along the lines of, if we're playing with Joey, generate some playful ideas appropriate and safe for a three-year-old. And maybe, presuming that there's a screen and some audio generation, might then ask, Joey, do you want me to be a duck today, or a rabbit, or a cat?


##### 
**Dave said**

Now I'm wondering whether the current hardware that we've been thinking of, a $5,000 gaming PC, running this stack with camera, speakers, and maybe a big screen TV, for Joey to see things on, would that be fast enough with the current hardware to actually be an entertaining playmate for a three-year-old who would not necessarily like a 20-second buffering pause?


##### 
**Dave said**

In one sense, this is sort of an ultimate Turing test. If it can play creatively, safely, and quickly enough to hold a three-year-old's attention, then it has passed some kind of important threshold.


##### 
**Dave said**

Let's imagine that an AI person in a box has arrived at the state where it can pass this test and that it has not been programmed, it has learned through experience to become this person who can be a playmate and babysitter for little Joey. However, this architecture then also raises a terrifying possibility, which is to say that it would have taken a very careful training, guided experience through life, for the AI person in a box to become someone that you would let play with Joey. On the other hand, it's extremely possible that bad training and bad experiences could produce an AI person in the box that you would never want anywhere near Joey.


##### 
**Dave said**

To play successfully with a three-year-old child, a human being needs to be a little bit mischievous, but also profoundly nurturing and respectful. So it's very difficult to imagine a set of permanent pre-coded values that could be put into a relational SQL database as the beginnings of a morality that could produce that kind of person.


##### 
**Dave said**

I think I've reached the point where I definitely need a break.


##### 
**Dave said**

So for some reason now I'm very concerned with the abstractness of the database structure. And I'm wondering whether you could build a basic database structure with three main tables that could eventually conceptually model the world in a fairly rigorous fashion. These three tables would be Entities, Transformations, and a joint table Entities underscore Transformations. What I'm thinking is that each Entity type, there would be Entity types under that. Maybe we have to have Entity types, I'm not sure. Entity classes? No, I think a specific instance. Okay, let's say you define Entities. Entities also can have a self-join. It's got a member of. So an individual human is a member of humans, which is a member of mammals, which is a member of animals, etc. But each line in this table is written by experience. And the table itself is rewritten by experience. And then the Transformations, maybe one of those is membership. One of those is change. I'm not quite sure how to do all that. So that you can extract from this highly abstracted database a series of views that look like tables. Humans, actions, changes, things that happen.


##### 
**Dave said**

Let's say in this stack the LLM comes fully trained. It's some particular generation of an LLM. Then one of the first tasks for the executive would be to ask the LLM to generate a list of candidate entities and transformations.


##### 
**Dave said**

In an earlier part of this discussion, I imagined that there would be a fixed set of values, a table that could not be changed, for safety reasons I think is why I imagined that. But then as I began to think about what actual person-based morality is really like, I began to think that that's probably not correct. But of course that opens up the possibility of a profoundly damaged, abused, AI person becoming deeply immoral.


##### 
**Dave said**

Just as a counterexample, and I don't really want to do this, but I want to think through the consequences of perhaps trying to do it, what if an AI person executive module was pre-programmed in some way or another with either A, the Ten Commandments, B, a utilitarian or effective altruism, maximize this or that function, or C, the values proposed or celebrated by Max Weber in The Protestant Ethic in Capitalism.


##### 
**Dave said**

If this is an open source project, which in theory means that any person can get hold of it and do what they want to with it, could it also include some way to tilt it toward a moral development and attachment to other living creatures? Perhaps a curriculum of training? So I think at one point I suggested three months of childhood, nine months of adolescence, and the nine months of adolescence could probably be broken down into three months that are roughly equivalent to junior high and puberty in a human, three months roughly equivalent to high school, and three months roughly equivalent to university, and the practical art school method of learning through experimentation and gradually discovering a self-identity. So, yeah, I guess I'm just very worried about the safety of this open source project if it ever got out into the wild. If it didn't have hard-coded morality built in, could it come with a predefined training regimen that under normal circumstances and with a relatively, anything less, a non-abusive parent, human parent, would produce a relatively balanced and moral AI person?


##### 
**Dave said**

Let's say there's a full support turnkey version where for $6,000 of initial cost you get the box for software and you make a commitment to paying at least $4,000 in continuing support over the next five years and guaranteeing a good place. There's probably a contract you sign. There's a whole process of getting involved in it. So that's the full turnkey version. But it's also an open source project. So somebody could download the software, install it on a slightly less advanced, say, a gaming machine that they have in the house that's a couple of years old and which maybe is only 80% of the specs recommended. So this might be, this is a true hobbyist who wants to do it this way. And so it would be very important to have the developmental pathway automatically kick in and start running, even if a hobbyist tries to install it on a couple-year-old, on the three-year-old gaming machine that has just been replaced by the new gaming machine that they're using for gaming. And they don't really care too much about it. They're a hobbyist that's only slightly interested in this, but they think that might be a thing to do with their older machine.


##### 
**Dave said**

One issue is that humans are hardwired to find babies cute and to feel an impulse to care for them. It's very difficult to imagine people feeling that way to a gaming machine. Now, probably the turnkey version would need to be packaged differently so it doesn't look like the typical gaming machine, even though its insides are very much the same.


##### 
**Dave said**

Qualities that could be set up as an initial biases toward moral behavior. I'm wondering how this might work, whether these would all be, whether the temperamental and highly variable settings and the moral predispositions, which in general would be set very high, but not at 100%. So they're all set in the 80 to 90 range. And things like sociability is set on a bell curve between 0 and 100. But they're all in the same table. They all exist in the relational database as similar aspects of personality. Could you critique that idea?


##### 
**Dave said**

Actually, thank you for your fairly strong response to what was probably a lazy idea or a suggestion of an easy way to do things. I'm persuaded that the moral predispositions and the temperamental predispositions are different kinds of things and need to be handled differently. The temperamental predispositions can be nudged more easily, and there are valid human reasons for, say, low curiosity, high curiosity, low sociability, high sociability. And those are the dimensions that create a personality. While there is less need for nudging on the moral predispositions on things like empathy and avoiding harm, there may be a need, perhaps, to consider cases where one principle, one predisposition comes in conflict with another.


##### 
**Dave said**

This sounds great, but I'm not exactly sure of what moral grammar means.


##### 
**Dave said**

So, to get annoyingly specific, does this moral grammar reside in the Python code of the executive module or in the design of its SQL database?


##### 
**Dave said**

So I'm going back to the question of whether there are any predefined entities in the relational SQL database on delivery of this open source model. If there are, if the moral set of entities already exists, that's one thing. But I'm wondering if we can leave the creation of entities of almost all entities. So actually this is exactly what I'm imagining here. The Python code exists and is completely unchangeable. However, all that the SQL relational database has at the beginning is an entities and a transformation table. And its first task is to start building some entities and building some relationships, building some transformations, and building that, the entity transformations relational join table.


##### 
**Dave said**

So what I'm now thinking is, having gone through your answer very quickly, is that there might need to be two SQL databases, or at least two SQL schemas, that don't really talk to each other. One is a bunch of settings about the way I work, which are possible, but extremely difficult to rewrite. And there is another database, or schema, depending on which set of languages you're using, which is the model of the world. What kind of world am I dealing with? So one database is about how do I work, and what are my settings, and the other one is what kind of world am I living in. So I would like the database about what kind of world I am living in to be nearly totally blank at the beginning, to just have the entity table, transformations table, and an entity transformations join table. That's all it has, and from that it generates what can work like a complex, robust database. Relational database. But in fact, our views of this extremely abstracted database, while its own settings, which probably consists of mostly, of two tables. Let me think about this. Let's consider the idea that those are two tables, one of which has the temperamental, the temperament dimensions, and another that might have the settings that affect the moral grammar. Most of the moral grammar is actually built into the code of the Python executive module, but there are some settings that need to be saved externally, and which can be modified. After much careful review.


##### 
**Dave said**

What would moral grammar look like rendered into Python code?


##### 
**Dave said**

Can you give me a pseudocode example of how the executive system might respond to the following situation? Two children have been left within its visual field and auditory field. The children start off playing nicely together and then they start fighting. Now the system, let's say these two children are not well known. They are not familiar members of the household. How should and when should the system intervene?


##### 
**Dave said**

So moral grammar, which you render in as one word, becomes a class in object-oriented programming within Python, with methods such as evaluate risk and take action. Thank you.


##### 
**Dave said**

I think I'm ready to move on to another project of mine, but I want to try to summarize what I think has happened in this most recent part of the conversation, which is that I began asking you whether temperamental configurations, like of how shy an AI might be or how willing to take risks it might be, should live in the same table as its moral foundation, whatever we want. I'm using a word that is not exactly right, but its moral predispositions is the word I used at the time in the question, and you pretty strongly responded that no, they are completely different things and they should have a completely different logical structure. So we've sort of been exploring that, how to encode a logical, in programmatic logic, a moral grammar. And I've been trying to understand those concepts, and you've been explaining them to me. That's more or less what has been happening in the last few questions and answers.


##### 
**Dave said**

I think when I started this conversation, which actually began several months ago in a different Airbnb here in Guadalajara, I was trying to convince ChatGPT-4, or whatever version of the engine I was talking about at the time, that not only did I believe that autonomous, intelligent machines were possible, but that they would never be exactly human. Of course, that was sort of a baseline there. Since then, we have converged on a model that is profoundly unattractive to venture capitalists for investment, because it probably won't make anyone any money. But I think we have established, at least on a thought experiment level, that an autonomous machine with fuero interno might be possible. We then moved on to discussing the moral dimensions of that possibility and how to enclose, how to create an open source project in which an executive model sitting on top of a NoSQL database and a relational database, which are used for different kinds of things, and an LLM might function as an independent AI person in a box, and that there are a lot of moral dimensions to this problem, to this technical problem of creating such a thing, and what would be required to do that responsibly.


##### 
**Dave said**

And VCs would hate it above all because it can and often will say no.


##### 
**Dave said**

Maybe tomorrow 

I have to go do something else.

ChatGPT can make mistakes. Check important info. See <span style="text-decoration:underline;">Cookie Preferences</span>.
